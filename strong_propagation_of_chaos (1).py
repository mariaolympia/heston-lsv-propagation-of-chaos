# -*- coding: utf-8 -*-
"""strong-propagation-of-chaos

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cytUuwJAgZst_69GrXBbdULNV_cl9xqz
"""

# --- README.md ---
readme_text = """# Calibrated Heston-type LSV Model - Strong Propagation of Chaos Experiments

This repository contains Python code that simulates a **calibrated Heston-type Local Stochastic Volatility (LSV) model** and studies the **strong propagation of chaos** phenomenon.

## Overview

The code simulates a **calibrated Heston-type Local Stochastic Volatility (LSV) model**, where the squared volatility process follows **Cox–Ingersoll–Ross (CIR) dynamics**. To approximate the conditional expectation, it uses the **Nadaraya–Watson kernel estimator**.

Instead of relying on real market data, the code generates a synthetic **market implied volatility surface** using **already calibrated Heston parameters** obtained from an FX market. From this, a **Dupire local volatility surface** is generated and extended via **bicubic interpolation and extrapolation**.

The implementation uses **QuantLib**, an open-source library for quantitative finance, for model construction (Heston), curve setup, pricing engines, path generation, and building both implied and local volatility surfaces.

The main goal is to investigate **strong propagation of chaos** by simulating large interacting particle systems and analysing convergence rates under different sets of model parameters.


## Features

- Artificial market surface from calibrated Heston parameters
- Dupire local volatility surface generation
- Euler-Maruyama scheme for stock price process
- Full truncation Euler scheme for variance positivity
- Nadaraya-Watson kernel regression to approximate conditional expectation
- Strong propagation of chaos experiments

## Requirements

- Python 3.9+
- numpy
- pandas
- matplotlib
- scipy
- seaborn
- scikit-learn
- QuantLib

Install dependencies with:
pip install -r requirements.txt

## Usage

Run the simulation:
python strong_propagation_of_chaos.py

You can also import the functions into another Python script:
from strong_propagation_of_chaos import run_experiment
results = run_experiment(do_plot=True)

## Output

- Local volatility surface plots
- Implied volatility surface plots
- Plots of "market prices" over strikes for fixed maturity
- Plots illustrating the strong propagation of chaos (i.e. convergence of the particle system as the number of particles increases)

## License

This project is licensed under the MIT License – see the LICENSE file for details.
"""

with open("/content/README.md", "w") as f:
    f.write(readme_text)

# --- requirements.txt ---
requirements_text = """numpy
pandas
matplotlib
scipy
QuantLib
"""

with open("/content/requirements.txt", "w") as f:
    f.write(requirements_text)

# --- .gitignore ---
gitignore_text = """__pycache__/
*.pyc
.ipynb_checkpoints/
"""

with open("/content/.gitignore", "w") as f:
    f.write(gitignore_text)

print("README.md, requirements.txt, and .gitignore created in /content.")

from google.colab import files
files.download('/content/.gitignore')

import warnings
warnings.filterwarnings('ignore')
!pip install QuantLib
import QuantLib as ql
import numpy as np
import pandas as pd
import itertools
import math

from scipy.stats import norm
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.interpolate import CubicSpline, interp1d
from scipy.stats import linregress
from sklearn.kernel_ridge import KernelRidge

!pip install seaborn
import seaborn as sns
sns.set_theme(style="whitegrid")

######################################################################################################
# functions to plot 3D vol surfaces, generate paths, and generate vol surface from Heston parameters #
######################################################################################################

def plot_vol_surface(vol_surface, plot_years=np.arange(0.1, 3, 0.1), plot_strikes=np.arange(70, 130, 1), funct='blackVol'):
    if type(vol_surface) != list:
        surfaces = [vol_surface]
    else:
        surfaces = vol_surface

    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    X, Y = np.meshgrid(plot_strikes, plot_years)

    for surface in surfaces:
        method_to_call = getattr(surface, funct)

        Z = np.array([method_to_call(float(y), float(x))
                      for xr, yr in zip(X, Y)
                          for x, y in zip(xr,yr) ]
                     ).reshape(len(X), len(X[0]))

        surf = ax.plot_surface(X,Y,Z, rstride=1, cstride=1, linewidth=0.1)

    fig.colorbar(surf, shrink=0.5, aspect=5)

def generate_multi_paths_df(sequence, num_paths):
    spot_paths = []
    vol_paths = []

    for i in range(num_paths):
        sample_path = seq.next()
        values = sample_path.value()

        spot, vol = values

        spot_paths.append([x for x in spot])
        vol_paths.append([x for x in vol])

    df_spot = pd.DataFrame(spot_paths, columns=[spot.time(x) for x in range(len(spot))])
    df_vol = pd.DataFrame(vol_paths, columns=[spot.time(x) for x in range(len(spot))])

    return df_spot, df_vol

def create_vol_surface_mesh_from_heston_params(today, calendar, spot, v0, kappa, theta, rho, sigma,
                                               rates_curve_handle, dividend_curve_handle,
                                               strikes = np.linspace(40, 200, 161), tenors = np.linspace(0.1, 3, 60)):
    quote = ql.QuoteHandle(ql.SimpleQuote(spot))

    heston_process = ql.HestonProcess(rates_curve_handle, dividend_curve_handle, quote, v0, kappa, theta, sigma, rho)
    heston_model = ql.HestonModel(heston_process)
    heston_handle = ql.HestonModelHandle(heston_model)
    heston_vol_surface = ql.HestonBlackVolSurface(heston_handle)

    data = []
    for strike in strikes:
        data.append([heston_vol_surface.blackVol(tenor, strike) for tenor in tenors])

    expiration_dates = [calendar.advance(today, ql.Period(int(365*t), ql.Days)) for t in tenors]
    implied_vols = ql.Matrix(data)
    feller = 2 * kappa * theta - sigma ** 2

    return expiration_dates, strikes, implied_vols, feller

# World State for Vanilla Pricing
spot = 100
rate = 0.00
today = ql.Date(8, 7, 2024)
calendar = ql.NullCalendar()
day_count = ql.Actual365Fixed()

# Setting up the flat risk-free curves
riskFreeCurve = ql.FlatForward(today, rate, ql.Actual365Fixed())

#flat_ts = ql.YieldTermStructureHandle(riskFreeCurve)
#dividend_ts = ql.YieldTermStructureHandle(riskFreeCurve)

#EXTRAS
ql.Settings.instance().evaluationDate = today
flat_ts = ql.YieldTermStructureHandle(
    ql.FlatForward(today, rate, day_count))
dividend_ts = ql.YieldTermStructureHandle(
    ql.FlatForward(today, rate, day_count))


#############################################################################################################
# Firstly, create the artificial volatility surface "observed in the market" using already calibrated Heston parameters
#############################################################################################################

dates, strikes, vols, feller = create_vol_surface_mesh_from_heston_params(today, calendar, spot, 0.0094, 1.4124, 0.0137, -0.1194, 0.3, flat_ts, dividend_ts)

market_data = ql.BlackVarianceSurface(today, calendar, dates, strikes, vols, day_count)

plot_vol_surface(market_data)
plt.title('Artificial "market" volatility surface')

# For given maturity 1Y, plot the volatility against Strikes
v0, kappa, theta, rho, sigma = 0.0094, 1.4124, 0.0137, -0.1194, 0.3

strikes = np.linspace(70,130,58)
tenors = np.arange(0.1, 3, 0.05)

quote = ql.QuoteHandle(ql.SimpleQuote(spot))
heston_process = ql.HestonProcess(flat_ts, dividend_ts, quote, v0, kappa, theta, sigma, rho)
model = ql.HestonModel(heston_process)
heston_handle = ql.HestonModelHandle(model)
heston_vol_surface = ql.HestonBlackVolSurface(heston_handle)

data = []
for strike in strikes:
    data.append([heston_vol_surface.blackVol(tenor, strike) for tenor in tenors])

expiration_dates = [calendar.advance(today, ql.Period(int(365*t), ql.Days)) for t in tenors]
implied_vols = ql.Matrix(data)
feller = 2 * kappa * theta - sigma ** 2

# black_var_surface = ql.BlackVarianceSurface(
#     today, calendar,
#     expiration_dates, strikes,
#     implied_vols, day_count)

strikes = np.linspace(40, 200, 161)
strikes_grid = np.arange(strikes[0], strikes[-1],10)
expiry = 1.0 # years
# implied_vols = [market_data.blackVol(expiry, s)
#                 for s in strikes_grid] # can interpolate here
implied_vols = [heston_vol_surface.blackVol(1, s) for s in strikes_grid]
fig, ax = plt.subplots()
ax.plot(strikes_grid, implied_vols, label="Implied Volatility Surface")
ax.set_xlabel("Strikes", size=12)
ax.set_ylabel("Volatilities", size=12)
legend = ax.legend(loc="upper right")

plt.show()

#####################################################################################################
# Construct the European Option (these are the true values that we will be comparing our results to)
#####################################################################################################

option = ql.Option.Call
strikes = np.linspace(80,120,101)
expiration= calendar.advance(today, ql.Period(int(365), ql.Days))
prices=[]
for s in strikes:
    payoff = ql.PlainVanillaPayoff(option, s)
    exercise = ql.EuropeanExercise(expiration)
    european_option = ql.VanillaOption(payoff, exercise)
    v0, kappa, theta, rho, sigma = 0.0094, 1.4124, 0.0137, -0.1194, sigma
    tenors = np.arange(0.1, 3, 0.05)

    quote = ql.QuoteHandle(ql.SimpleQuote(spot))
    heston_process = ql.HestonProcess(flat_ts, dividend_ts, quote, v0, kappa, theta, sigma, rho)
    model = ql.HestonModel(heston_process)

    european_option.setPricingEngine(ql.AnalyticHestonEngine(model))
    hs_price = european_option.NPV()
    prices.append(hs_price)

################################################################
# STORE THE GENERATED VALUES OF THE STOCK AS A PANDAS DATAFRAME
################################################################

length = 1
n_timesteps = [5,10,20,40,80,160]
list_stock_prices = []
for timestep in n_timesteps:
    times = ql.TimeGrid(length, timestep)
    dimension = heston_process.factors()

    rng = ql.GaussianRandomSequenceGenerator(ql.UniformRandomSequenceGenerator(dimension * timestep, ql.UniformRandomGenerator()))
    seq = ql.GaussianMultiPathGenerator(heston_process, list(times), rng, False)

    df_spot, df_vol = generate_multi_paths_df(seq, 1)
    df_spot.head()

    stock_prices = df_spot.values.tolist()
    list_stock_prices.append(stock_prices)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(strikes.reshape(-1, 1))
mymodel_1 = np.poly1d(np.polyfit(strikes,prices, 2))
myline_1 = np.linspace(80, 120, 161)
plt.scatter(strikes,prices)
plt.plot(myline_1, mymodel_1(myline_1))
plt.title('T = 1Y')
plt.xlabel('Strikes')
plt.ylabel('Market Prices')

plt.show()

#######################################################
# Calculate the Dupire volatility
#######################################################

spot_quote = ql.QuoteHandle(ql.SimpleQuote(spot))

market_data.setInterpolation("bicubic")
local_vol_handle = ql.BlackVolTermStructureHandle(market_data)
local_vol = ql.LocalVolSurface(local_vol_handle, flat_ts, dividend_ts, spot_quote)
local_vol.enableExtrapolation()

# Plot the Dupire surface
plot_vol_surface(local_vol, funct='localVol')
plt.title('Local volatility surface')

###################################################################################################
# Create pure Heston model using calibrated params (here perturbed). We aim to fix this difference.
###################################################################################################

v0, kappa, theta, rho, sigma, spot = 0.0094, 1.5, 0.01, -0.1, 0.3, 100
feller = 2 * kappa * theta - sigma ** 2

heston_process = ql.HestonProcess(flat_ts, dividend_ts, spot_quote, v0, kappa, theta, sigma, rho)
heston_model = ql.HestonModel(heston_process)

# How does the vol surface look at the moment?
heston_handle = ql.HestonModelHandle(heston_model)
heston_vol_surface = ql.HestonBlackVolSurface(heston_handle)

# Plot the vol surface
plot_vol_surface([market_data, heston_vol_surface])
plt.title('Implied (blue) and Heston (orange) volatility surfaces')

###################################################################################################
# We now define some functions that will be useful for the implementation of the particle method
###################################################################################################

def gauss_const(epsilon):
    return 1/(epsilon*np.sqrt(np.pi*2))

def gauss_exp(Xi, Xj, epsilon):
    num =  - 0.5*np.square(np.add(Xj,-Xi))
    den = epsilon*epsilon
    return num/den

def kernel_function(epsilon,Xi,Xj):
    const = gauss_const(epsilon)
    gauss_val = const*np.exp(gauss_exp(Xi,Xj,epsilon))
    return gauss_val

def g(Y):
    return np.exp(Y)

def g_squared(Yi):
    return np.square(np.exp(Yi))

def conditionalexpectation(KernelFunction,g_squared,delta):
    numerator = np.sqrt(np.sum(KernelFunction)+delta)
    S = np.multiply(KernelFunction,g_squared)
    denomenator = np.sqrt(np.sum(S)+delta)
    return numerator/denomenator

# def dW(delta_t: float) -> float:
    ##dW = np.random.normal(loc=0.0, scale=np.sqrt(delta_t))
    ##return np.full((1, n_particles), dW)

########################################################################################################
# We now test the strong propagation of chaos for different Feller ratios by considering different kappa
########################################################################################################

M = 100
n_particles = [40,80,160,320,640,1280,2560,5120,10240]
delta = 0.01
h_1 = 10
t_0 = 0
T  = 1.0

results_by_kappa = {}

for kappa in [1.5, 6, 18]:

    v0, kappa, theta, rho, sigma, r, spot = 0.0094, kappa, 0.01, -0.1, 0.3, 0, 100

    for n in range(len(n_particles)):
        dt = (T - t_0)/M
        tk = np.arange(t_0, T+dt, dt)
        err_em = np.zeros((n_particles[n], len(tk)))

        particles_X_values = spot * np.ones(n_particles[n])
        particles_Y_values = v0 * np.ones(n_particles[n])

        data = np.ones((M+1, n_particles[n]))
        X_k = np.ones((M+1, n_particles[n]))
        Y_k = np.ones((M+1, n_particles[n]))

        volatility = np.ones((M+1, M+1))
        X_k[0] = particles_X_values
        Y_k[0] = particles_Y_values

        lsv_i = np.ones(M+1)
        strong_errors = []
        for k in range(1, M+1):
            t_k = tk[k]
            Xj = X_k[k - 1]
            Yj = Y_k[k - 1]
            dW1 = np.random.normal(0.0, np.sqrt(dt), len(Yj))
            dW2 = rho * dW1 + math.sqrt(1 - rho ** 2) * dW1

            for i in range(n_particles[n]):
                Xi = np.full((1, n_particles[n]), particles_X_values[i])
                th = np.full((1, len(Yj)), theta)

                alpha_i = conditionalexpectation(kernel_function(h_1, Xi, Xj), (Yj), delta)

                tenor = t_k
                strike = particles_X_values[i]
                sigma_dup = np.abs([local_vol.localVol(tenor, strike)][0])

                a = np.multiply(particles_X_values[i], alpha_i) * sigma_dup
                b = np.multiply(np.sqrt(particles_Y_values[i]), dW1[i])
                c = np.multiply(a, b)

                X_k[k][i] = particles_X_values[i] + (r * particles_X_values[i]) * dt + c

            particles_X_values = X_k[k]
            th = np.full((1, len(Y_k[k - 1])), theta)
            Y_k[k] = np.abs(Y_k[k - 1] + kappa * (np.add(th, -Y_k[k - 1])) * dt + sigma * np.sqrt(Y_k[k - 1]) * dW2)
            particles_Y_values = Y_k[k]

    strong_error_list_1 = []

    for n in range(len(n_particles)):
        dt = (T - t_0)/M
        tk = np.arange(t_0, T+dt, dt)
        error = np.zeros((n_particles[n], len(tk)))

        particles_X2_values = spot * np.ones(n_particles[n])
        particles_Y2_values = v0 * np.ones(n_particles[n])

        data2 = np.ones((M+1, n_particles[n]))
        X2_k = np.ones((M+1, n_particles[n]))
        Y2_k = np.ones((M+1, n_particles[n]))

        volatility = np.ones((M+1, M+1))
        X2_k[0] = particles_X2_values
        Y2_k[0] = particles_Y2_values

        lsv_i = np.ones(M+1)
        strong_errors = []
        for k in range(1, M+1):
            t_k = tk[k]
            X2j = X2_k[k - 1]
            Y2j = Y2_k[k - 1]
            dW1 = np.random.normal(0.0, np.sqrt(dt), len(Y2j))
            dW2 = rho * dW1 + math.sqrt(1 - rho ** 2) * dW1

            for i in range(n_particles[n]):
                X2i = np.full((1, int(n_particles[n] * 0.5)), particles_X2_values[i])
                Y2_j = Y2j[int(n_particles[n] * 0.5):]
                th = np.full((1, len(Y2j)), theta)
                alpha_i = conditionalexpectation(kernel_function(h_1, X2i, X2j[int(n_particles[n] * 0.5):]), (Y2_j), delta)

                tenor = t_k
                strike = particles_X2_values[i]
                sigma_dup = np.abs([local_vol.localVol(tenor, strike)][0])

                a = np.multiply(particles_X2_values[i], alpha_i) * sigma_dup
                b = np.multiply(np.sqrt(particles_Y2_values[i]), dW1[i])
                c = np.multiply(a, b)

                X2_k[k][i] = particles_X2_values[i] + (r * particles_X2_values[i]) * dt + c
                error[i, k] = (X2_k[k][i] - X_k[k][i])**2

            av_err_em = np.sum(error, axis=1) / n_particles[n]
            strong_errors.append(av_err_em)
            particles_X2_values = X2_k[k]

            th = np.full((1, len(Y2_k[k - 1])), theta)
            Y2_k[k] = np.abs(Y2_k[k - 1] + kappa * (np.add(th, -Y2_k[k - 1])) * dt + sigma * np.sqrt(Y2_k[k - 1]) * dW2)
            particles_Y2_values = Y2_k[k]

        strong_error_list_1.append(max(strong_errors[0]))

    results_by_kappa[kappa] = strong_error_list_1

x_vals = [40,80,160,320,640,1280,2560,5120,10240]
xlog = np.log(x_vals)

# Reference lines
def refline_func(b):
    return -0.4 * b + 0.7

def refline2_func(b):
    return -0.5 * b + 0.8

refline = list(map(refline_func, xlog))
refline2 = list(map(refline2_func, xlog))

plt.plot(xlog, refline, label='-0.25 reference slope', linewidth=1.2, linestyle='dotted', color='purple')
plt.plot(xlog, refline2, label='-0.50 reference slope', linewidth=1.2, linestyle='dotted', color='red')

colors = {1.5: 'blue', 6: 'orange', 18: 'green'}

for kappa, error_list in results_by_kappa.items():
    df_strong = pd.DataFrame({
        'x': x_vals,
        'y': np.sqrt(error_list)
    })

    ylog = np.log(df_strong['y'])

    slope, intercept, r_value, p_value, std_err = linregress(xlog, ylog)
    print(f"Rate when κ={kappa} is {slope:.4f}")

    mymodel = slope * xlog + intercept
    plt.plot(xlog, mymodel, label=f'slope κ={kappa} = {slope:.2f}', linewidth=1.2, linestyle='solid', color=colors[kappa])

plt.title('Strong propagation of chaos')
plt.xlabel('log(N)')
plt.ylabel('log(Error)')
plt.legend(loc=3)
plt.show()